{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the notebook reloads the module each time we modify it\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# make sure the displays are nice\n",
    "# %matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make plots look nice\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"ticks\")\n",
    "\n",
    "# Import generic libraries\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import pickle as pkl # needed to save and load python objects as byte strings.\n",
    "from IPython.core.display import display, HTML # needed to display HTML text\n",
    "import time\n",
    "\n",
    "# Import sklearn tools\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding topics in machine learning papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JMLR is one the main journals in machine learning. In this practical session, we'll look at abstracts of papers from JMLR, and we'll use a topic model to automatically extract topics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../data/jmlr.pkl\", 'rb') as f:\n",
    "    jmlr_papers = pkl.load(f) # loads a list of Python dictionaries. \n",
    "    number_of_papers = len(jmlr_papers)\n",
    "    print(\"Just loaded \"+str(number_of_papers)+\" papers from JMLR.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each entry of the list is a Python dictionary, and corresponds to an abstract.\n",
    "jmlr_papers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is a random abstract from JMLR, printed nicely\n",
    "index = npr.randint(number_of_papers)\n",
    "display(HTML(jmlr_papers[index]['abstract']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at data\n",
    "Let's try to isolate the papers on Bayesian ML and neural networks, to see which proportion of JMLR both topics actually represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayesian_jmlr_papers = []\n",
    "neural_network_jmlr_papers = []\n",
    "for paper in jmlr_papers:\n",
    "    bayesian_keywords = [\"Bayesian\", \"variational Bayes\", \"Carlo\", \"MCMC\"]\n",
    "    if any(kwd in paper[\"abstract\"] for kwd in bayesian_keywords):\n",
    "        bayesian_jmlr_papers.append(paper)\n",
    "    neural_network_keywords = [\"neural net\", \"Neural net\", \"deep\", \"Deep\"]\n",
    "    if any(kwd in paper[\"abstract\"] for kwd in neural_network_keywords):\n",
    "        neural_network_jmlr_papers.append(paper)\n",
    "        \n",
    "number_of_jmlr_papers = len(jmlr_papers)\n",
    "number_of_Bayesian_jmlr_papers = len(bayesian_jmlr_papers)   \n",
    "print(\"There are\", str(len(neural_network_jmlr_papers))+\" neural network papers out of\", number_of_jmlr_papers)\n",
    "print(\"There are\", str(number_of_Bayesian_jmlr_papers)+\" Bayesian papers out of\", number_of_jmlr_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian networks may not be about Bayesian statistics\n",
    "print(\"Out of which the number of papers about Bayesian networks is\")\n",
    "print(np.sum([\"Bayesian network\" in paper[\"abstract\"] for paper in bayesian_jmlr_papers]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot proportion of Bayesian papers in JMLR\n",
    "volumes = [int(paper[\"volume\"]) for paper in jmlr_papers] + [20]\n",
    "bins = np.unique(volumes) - .5\n",
    "plt.hist([int(paper[\"volume\"]) for paper in jmlr_papers], color=\"blue\", bins=bins, alpha=.3)\n",
    "bins = np.unique([int(paper[\"volume\"]) for paper in jmlr_papers] + [20]) - .5\n",
    "plt.hist([int(paper[\"volume\"]) for paper in bayesian_jmlr_papers], color=\"red\", bins=bins)\n",
    "plt.xticks([5*i for i in range(6)])\n",
    "plt.xlabel(\"JMLR volume\")\n",
    "plt.ylabel(\"Number of papers\")\n",
    "plt.xlim([0,20])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run sklearn's Variational Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `sklearn`to preprocess the data. Take the 1000 most frequent words in the corpus, not counting English stop words, and perform one-hot encoding. The resulting matrix will be very sparse. You should output an $N\\times d$ sparse matrix `tf` of `int`egers, where $N$ is the number of JMLR abstracts, and $d=1000$. Check out `CountVectorizer` from `sklearn`, it does all these things for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Extract tf features (term frequencies, i.e., raw term counts)\n",
    "tf = # TBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf # this should print something close to the following:\n",
    "     # <1898x1000 sparse matrix of type '<class 'numpy.int64'>'\n",
    "\t # with 85804 stored elements in Compressed Sparse Row format>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now fit the LDA to `tf`, say with $10$ topics for starters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Fit the LDA using online VB\n",
    "# Check Section 27.3.7 of [Murphy, 2012]for the definition of online VB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Print the top words in each topic, comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Pick an abstract and assign topics to its words. What is your loss function for that action?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running ADVI\n",
    "Fit the count version of LDA using ADVI and pyMC3. This requires you to decide on how to transform the variables in the model to unconstrained real variables. You can take inspiration from [this tutorial](https://docs.pymc.io/notebooks/lda-advi-aevb.html). Compare your results with sklean's online VB: what do you observe? Why do you think this is the case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus exercises\n",
    "* *Bonus exercise*: play around with topic extraction. Can you recognize topics? Find topics within Bayesian or neural network papers.\n",
    "* *Bonus exercise* if you like web scraping: get more data, say NeurIPS papers.\n",
    "* *Bonus exercise* if you like variational Bayes: implement your own variational Bayes class, without `sklearn`.\n",
    "* *Bonus exercise* implement collapsed Gibbs sampling on, say, a small dataset of 5 abstracts. \n",
    "* *Bonus exercise* if you enjoy slow MCMC algorithms like me: implement the collapsed Gibbs sampler, say for a small dataset of 5 abstracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
