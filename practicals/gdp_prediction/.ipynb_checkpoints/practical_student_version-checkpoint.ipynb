{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the notebook reloads the module each time we modify it\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Uncomment the next line if you want to be able to zoom on plots\n",
    "# %matplotlib notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import parametric_regression_student_version as pr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"ticks\")\n",
    "import numpy as np\n",
    "import pymc3 as pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data and utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've prepared a function to generate regression data\n",
    "$$\n",
    "y \\sim \\mathcal{N}(X\\theta_{\\text{true}}, \\sigma^2 I). \n",
    "$$\n",
    "Check how $\\theta_{\\text{true}}$ is generated, with a few ``support variables\", and the rest of the coordinates close to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = 100\n",
    "sample_size = 50\n",
    "X, y, theta_true, sigma_noise, indices_support = pr.generate_data(sample_size = sample_size, \n",
    "                                                        dimension = dimension, seed = 3)\n",
    "plt.plot(X[:,indices_support[0]], y, '+')\n",
    "plt.plot(X[:,0], y, 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've prepared a function to plot an estimator and the corresponding error bars, and compare it to the underlying true $\\theta_{\\text{true}}$ in green. The bottom plot is the same, but with $\\theta_{\\text{true}}$ substracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_std = .5*np.ones(theta_true.shape)\n",
    "fake_theta_hat = np.zeros(theta_true.shape)\n",
    "fig, ax = plt.subplots(2, figsize=(10,8))\n",
    "pr.plot_coefficients(ax, color=\"blue\", theta_true=theta_true, indices_support=indices_support, \n",
    "                     theta_hat=fake_theta_hat, lower_bound=fake_theta_hat-fake_std, \n",
    "                     upper_bound=fake_theta_hat+fake_std)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using sklearn's lasso implementation as baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your first exercise is to get scikit-learn's lasso estimate as a baseline. Go to the companion Python file, and fill in the corresponding function. Then plot your estimate and comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_lasso = pr.get_sklearn_lasso_estimate(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, figsize=(10,8))\n",
    "pr.plot_coefficients(ax, theta_true=theta_true, indices_support=indices_support, theta_hat=theta_lasso, label=\"lasso\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian lasso using a Laplace prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You second exercise is to use `pymc3` to implement Bayesian linear regression with a Laplace prior. Again, fill in the corresponding function in the companion Python file. You should output a `pymc3` Trace object, with the $\\theta$ variable of our regression problem named `theta`. Play around with different MCMC kernels.\n",
    "\n",
    "*Hint*: Remember, the Laplace prior is the one that makes the MAP estimator be the solution to the frequentist Lasso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = pr.get_mcmc_sample_for_laplace_prior(X, y)\n",
    "accept = np.sum(trace['theta'][1:,0] != trace['theta'][:-1,0])\n",
    "print(\"Acceptance Rate: \", accept/trace['theta'][:,0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check a summary of that trace, find out what the columns mean in the doc\n",
    "pm.summary(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot here a few trajectories of the chain corresponding to a variable in and out of support. \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your next exercise is to define the Bayesian estimator $\\hat\\theta_B$ for the squared loss, along with two vectors of length $d$ as well. They should contain the lower limit and the upper limit, respectively, of a 95% credible interval for each component of $\\theta$. I've filled the arrays with zeros for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_laplace = np.zeros((dimension,)) # not expected to show much sparsity\n",
    "lower_bound_laplace = np.zeros((dimension,))\n",
    "upper_bound_laplace = np.zeros((dimension,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(2, figsize=(10,8))\n",
    "pr.plot_coefficients(ax, theta_true, indices_support=indices_support, theta_hat=theta_laplace, lower_bound=lower_bound_laplace, upper_bound=upper_bound_laplace,\n",
    "             color=\"blue\", label=\"laplace\")\n",
    "pr.plot_coefficients(ax, theta_true, indices_support=indices_support, theta_hat=theta_lasso, color=\"red\", label=\"lasso\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Laplace prior usually tends to overregularize. Can you get rid of that problem playing with the parameters of the prior?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian lasso using the horseshoe prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll try to enforce some sparsity using a different prior, the ``horseshoe prior\" of [Carvalho et al. '10](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&ved=2ahUKEwj11da8iZvnAhWJz4UKHZSXCccQFjABegQIBBAB&url=ftp%3A%2F%2Fwebster.stat.duke.edu%2Fpub%2FWorkingPapers%2F08-31.pdf&usg=AOvVaw09RPSqHPGq9kyfLBNsMJE2). This will force the whole posterior to put mass on sparse vectors, unlike the Laplace prior. Your next exercise is to repeat the steps of the Laplace prior for the horseshoe, and see what we gained/lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_horseshoe = pr.get_mcmc_sample_for_horseshoe_prior(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.summary(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, fill in the Bayesian estimator for the squared loss and credible intervals.\n",
    "theta_horseshoe = np.zeros((dimension,))\n",
    "lower_bound_horseshoe = np.zeros((dimension,))\n",
    "upper_bound_horseshoe = np.zeros((dimension,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This will plot your estimates so far for comparison\n",
    "f, ax = plt.subplots(2, figsize=(10,8))\n",
    "pr.plot_coefficients(ax, theta_true, indices_support=indices_support, theta_hat=theta_laplace, lower_bound=lower_bound_laplace, upper_bound=upper_bound_laplace,\n",
    "                     color=\"blue\", label=\"laplace\")\n",
    "pr.plot_coefficients(ax, theta_true=theta_true, indices_support=indices_support, theta_hat=theta_horseshoe, lower_bound=lower_bound_horseshoe, \n",
    "                     upper_bound=upper_bound_horseshoe, color=\"orange\", label=\"horseshoe\")\n",
    "#pr.plot_coefficients(theta_true, theta_lasso, color=\"red\", label=\"lasso\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian lasso using the Finnish horseshoe prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've made it so far, you've deserved to pick your last exercise: either \n",
    "* find a dataset of your liking to which you apply the previous estimators. \n",
    "* delve into more subtle priors, with, e.g. the Finnish horseshoe prior of [Piironen and Vehtari '17](https://arxiv.org/pdf/1707.01694.pdf). You can use the cells below.\n",
    "* Try to learn more about HMC and its NUTS variant, the flagship MCMC kernel of `pymc3`. How are the HMC parameters tuned in NUTS? Can you prove that the resulting Markov kernel leaves the target distribution invariant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_finnish_horseshoe = pr.get_mcmc_sample_for_finnish_horseshoe_prior(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_finnish_horseshoe = np.zeros((dimension,))\n",
    "lower_bound_finnish_horseshoe = np.zeros((dimension,))\n",
    "upper_bound_finnish_horseshoe = np.zeros((dimension,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(2, figsize=(10,8))\n",
    "pr.plot_coefficients(ax, theta_true, indices_support=indices_support, theta_hat=theta_laplace,\n",
    "                     color=\"blue\", label=\"laplace\")\n",
    "pr.plot_coefficients(ax, theta_true=theta_true, indices_support=indices_support, theta_hat=theta_horseshoe, lower_bound=lower_bound_horseshoe, \n",
    "                     upper_bound=upper_bound_horseshoe, color=\"orange\", label=\"horseshoe\")\n",
    "pr.plot_coefficients(ax, theta_true, indices_support=indices_support, theta_hat=theta_finnish_horseshoe, lower_bound=lower_bound_finnish_horseshoe, \n",
    "                     upper_bound=upper_bound_finnish_horseshoe, color=\"magenta\", label=\"finnish\")\n",
    "#pr.plot_coefficients(theta_true, theta_lasso, color=\"red\", label=\"lasso\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
