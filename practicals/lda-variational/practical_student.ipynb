{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the notebook reloads the module each time we modify it\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# make sure the displays are nice\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make plots look nice\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"ticks\")\n",
    "\n",
    "# Import generic libraries\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import pickle as pkl # needed to save and load python objects as byte strings.\n",
    "from IPython.core.display import display, HTML # needed to display HTML text\n",
    "import time\n",
    "\n",
    "# Import sklearn tools\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding topics in machine learning papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JMLR is one the main journals in machine learning. In this practical session, we'll look at abstracts of papers from JMLR, and we'll use a topic model to automatically extract topics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/jmlr.pkl\", 'rb') as f:\n",
    "    jmlr_papers = pkl.load(f) # loads a list of Python dictionaries. \n",
    "    number_of_papers = len(jmlr_papers)\n",
    "    print(\"Just loaded \"+str(number_of_papers)+\" papers from JMLR.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each entry of the list is a Python dictionary, and corresponds to an abstract.\n",
    "jmlr_papers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is a random abstract from JMLR, printed nicely\n",
    "index = npr.randint(number_of_papers)\n",
    "display(HTML(jmlr_papers[index]['abstract']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at data\n",
    "Let's try to isolate the papers on Bayesian ML and neural networks, to see which proportion of JMLR both topics actually represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayesian_jmlr_papers = []\n",
    "neural_network_jmlr_papers = []\n",
    "for paper in jmlr_papers:\n",
    "    bayesian_keywords = [\"Bayesian\", \"variational Bayes\", \"Carlo\", \"MCMC\"]\n",
    "    if any(kwd in paper[\"abstract\"] for kwd in bayesian_keywords):\n",
    "        bayesian_jmlr_papers.append(paper)\n",
    "    neural_network_keywords = [\"neural net\", \"Neural net\", \"deep\", \"Deep\"]\n",
    "    if any(kwd in paper[\"abstract\"] for kwd in neural_network_keywords):\n",
    "        neural_network_jmlr_papers.append(paper)\n",
    "        \n",
    "number_of_jmlr_papers = len(jmlr_papers)\n",
    "number_of_Bayesian_jmlr_papers = len(bayesian_jmlr_papers)   \n",
    "print(\"There are\", str(len(neural_network_jmlr_papers))+\" neural network papers out of\", number_of_jmlr_papers)\n",
    "print(\"There are\", str(number_of_Bayesian_jmlr_papers)+\" Bayesian papers out of\", number_of_jmlr_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian networks may not be about Bayesian statistics\n",
    "print(\"Out of which the number of papers about Bayesian networks is\")\n",
    "print(np.sum([\"Bayesian network\" in paper[\"abstract\"] for paper in bayesian_jmlr_papers]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot proportion of Bayesian papers in JMLR\n",
    "volumes = [int(paper[\"volume\"]) for paper in jmlr_papers] + [20]\n",
    "bins = np.unique(volumes) - .5\n",
    "plt.hist([int(paper[\"volume\"]) for paper in jmlr_papers], color=\"blue\", bins=bins, alpha=.3)\n",
    "bins = np.unique([int(paper[\"volume\"]) for paper in jmlr_papers] + [20]) - .5\n",
    "plt.hist([int(paper[\"volume\"]) for paper in bayesian_jmlr_papers], color=\"red\", bins=bins)\n",
    "plt.xticks([5*i for i in range(6)])\n",
    "plt.xlabel(\"JMLR volume\")\n",
    "plt.ylabel(\"Number of papers\")\n",
    "plt.xlim([0,20])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run sklearn's Variational Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `sklearn`to preprocess the data. Take the 1000 most frequent words in the corpus, not counting English stop words, and perform one-hot encoding. The resulting matrix will be very sparse. You should output an $N\\times d$ sparse matrix `tf` of `int`egers, where $N$ is the number of JMLR abstracts, and $d=1000$. Check out `CountVectorizer` from `sklearn`, it does all these things for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract tf features (term frequencies, i.e., raw term counts)\n",
    "# tf = ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf # this should print something close to the following:\n",
    "     # <1898x1000 sparse matrix of type '<class 'numpy.int64'>'\n",
    "\t # with 85804 stored elements in Compressed Sparse Row format>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now fit the LDA to `tf`, say with $10$ topics for starters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Fit the LDA using VB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Print the top words in each topic, comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Pick an abstract and assign topics to its words. What is your loss function for that action?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus exercises\n",
    "* *Bonus exercise*: play around with topic extraction. Can you recognize topics? Find topics within Bayesian or neural network papers.\n",
    "* *Bonus exercise* if you like web scraping: get more data, say NeurIPS papers.\n",
    "* *Bonus exercise* if you like variational Bayes: implement your own variational Bayes class, without `sklearn`.\n",
    "* *Bonus exercise* if you like slow MCMC algorithms: implement the collapsed Gibbs sampler, say with fixed alpha and beta for starters.\n",
    "* *Bonus question* why is VB somehow immune to label switching?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
