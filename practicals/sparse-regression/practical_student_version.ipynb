{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the notebook reloads the module each time we modify it\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Uncomment the next line if you want to be able to zoom on plots\n",
    "%matplotlib notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import parametric_regression_student_version as pr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"ticks\")\n",
    "import numpy as np\n",
    "import pymc3 as pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data and utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've prepared a function to generate regression data\n",
    "$$\n",
    "y \\sim \\mathcal{N}(X\\theta_{\\text{true}}, \\sigma^2 I). \n",
    "$$\n",
    "Check how $\\theta_{\\text{true}}$ is generated, with a few ``support variables\", and the rest of the coordinates close to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = 100\n",
    "sample_size = 50\n",
    "X, y, theta_true, sigma_noise, indices_support = pr.generate_data(sample_size = sample_size, \n",
    "                                                        dimension = dimension, seed = 3)\n",
    "plt.plot(X[:,indices_support[0]], y, '+')\n",
    "plt.plot(X[:,0], y, 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've prepared a function to plot an estimator and the corresponding error bars, and compare it to the underlying true $\\theta_{\\text{true}}$ in green. The bottom plot is the same, but with $\\theta_{\\text{true}}$ substracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_std = .5*np.ones(theta_true.shape)\n",
    "fake_theta_hat = np.zeros(theta_true.shape)\n",
    "fig, ax = plt.subplots(2, figsize=(10,8))\n",
    "pr.plot_coefficients(ax, color=\"blue\", theta_true=theta_true, indices_support=indices_support, \n",
    "                     theta_hat=fake_theta_hat, lower_bound=fake_theta_hat-fake_std, \n",
    "                     upper_bound=fake_theta_hat+fake_std)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using sklearn's lasso implementation as baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your first exercise is to get scikit-learn's lasso estimate as a baseline. Go to the companion Python file, and fill in the corresponding function. Then plot your estimate and comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_lasso = pr.get_sklearn_lasso_estimate(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, figsize=(10,8))\n",
    "pr.plot_coefficients(ax, theta_true=theta_true, indices_support=indices_support, theta_hat=theta_lasso, label=\"lasso\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian lasso using a Laplace prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You second exercise is to use `pymc3` to implement Bayesian linear regression with a Laplace prior. Again, fill in the corresponding function in the companion Python file. You should output a `pymc3` Trace object, with the $\\theta$ variable of our regression problem named `theta`. Play around with different MCMC kernels.\n",
    "\n",
    "*Hint*: Remember, the Laplace prior is the one that makes the MAP estimator be the solution to the frequentist Lasso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = pr.get_mcmc_sample_for_laplace_prior(X, y)\n",
    "accept = np.sum(trace['theta'][1:,0] != trace['theta'][:-1,0])\n",
    "print(\"Acceptance Rate: \", accept/trace['theta'][:,0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check a summary of that trace, find out what the columns mean in the doc\n",
    "pm.summary(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot here a few trajectories of the chain corresponding to a variable in and out of support. \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your next exercise is to define the Bayesian estimator $\\hat\\theta_B$ for the squared loss, along with two vectors of length $d$ as well. They should contain the lower limit and the upper limit, respectively, of a 95% credible interval for each component of $\\theta$. I've filled the arrays with zeros for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_laplace = trace.get_values('theta').mean(0) # not expected to show much sparsity\n",
    "lower_bound_laplace = np.quantile(trace.get_values('theta'), 0.025, axis=0)\n",
    "upper_bound_laplace = np.quantile(trace.get_values('theta'), 0.975, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(2, figsize=(10,8))\n",
    "pr.plot_coefficients(ax, theta_true, indices_support=indices_support, theta_hat=theta_laplace, lower_bound=lower_bound_laplace, upper_bound=upper_bound_laplace,\n",
    "             color=\"blue\", label=\"laplace\")\n",
    "pr.plot_coefficients(ax, theta_true, indices_support=indices_support, theta_hat=theta_lasso, color=\"red\", label=\"lasso\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Laplace prior usually tends to overregularize. Can you get rid of that problem playing with the parameters of the prior?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian lasso using the horseshoe prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll try to enforce some sparsity using a different prior, the ``horseshoe prior\" of [Carvalho et al. '10](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&ved=2ahUKEwj11da8iZvnAhWJz4UKHZSXCccQFjABegQIBBAB&url=ftp%3A%2F%2Fwebster.stat.duke.edu%2Fpub%2FWorkingPapers%2F08-31.pdf&usg=AOvVaw09RPSqHPGq9kyfLBNsMJE2). Your next exercise is to repeat the steps of the Laplace prior for the horseshoe, and see what we gained/lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_horseshoe = pr.get_mcmc_sample_for_horseshoe_prior(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.summary(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, fill in the Bayesian estimator for the squared loss and credible intervals.\n",
    "theta_horseshoe = np.zeros((dimension,))\n",
    "lower_bound_horseshoe = np.zeros((dimension,))\n",
    "upper_bound_horseshoe = np.zeros((dimension,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This will plot your estimates so far for comparison\n",
    "f, ax = plt.subplots(2, figsize=(10,8))\n",
    "pr.plot_coefficients(ax, theta_true, indices_support=indices_support, theta_hat=theta_laplace, lower_bound=lower_bound_laplace, upper_bound=upper_bound_laplace,\n",
    "                     color=\"blue\", label=\"laplace\")\n",
    "pr.plot_coefficients(ax, theta_true=theta_true, indices_support=indices_support, theta_hat=theta_horseshoe, lower_bound=lower_bound_horseshoe, \n",
    "                     upper_bound=upper_bound_horseshoe, color=\"orange\", label=\"horseshoe\")\n",
    "#pr.plot_coefficients(theta_true, theta_lasso, color=\"red\", label=\"lasso\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian lasso using the Finnish horseshoe prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've made it so far, you've deserved to pick your last exercise: either \n",
    "* find a dataset of your liking to which you apply the previous estimators. \n",
    "* delve into more subtle priors, with, e.g. the Finnish horseshoe prior of [Piironen and Vehtari '17](https://arxiv.org/pdf/1707.01694.pdf). You can use the cells below.\n",
    "* Try to learn more about HMC and its NUTS variant, the flagship MCMC kernel of `pymc3`. How are the HMC parameters tuned in NUTS? Can you prove that the resulting Markov kernel leaves the target distribution invariant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_finnish_horseshoe = pr.get_mcmc_sample_for_finnish_horseshoe_prior(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_finnish_horseshoe = np.zeros((dimension,))\n",
    "lower_bound_finnish_horseshoe = np.zeros((dimension,))\n",
    "upper_bound_finnish_horseshoe = np.zeros((dimension,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(2, figsize=(10,8))\n",
    "pr.plot_coefficients(ax, theta_true, indices_support=indices_support, theta_hat=theta_laplace,\n",
    "                     color=\"blue\", label=\"laplace\")\n",
    "pr.plot_coefficients(ax, theta_true=theta_true, indices_support=indices_support, theta_hat=theta_horseshoe, lower_bound=lower_bound_horseshoe, \n",
    "                     upper_bound=upper_bound_horseshoe, color=\"orange\", label=\"horseshoe\")\n",
    "pr.plot_coefficients(ax, theta_true, indices_support=indices_support, theta_hat=theta_finnish_horseshoe, lower_bound=lower_bound_finnish_horseshoe, \n",
    "                     upper_bound=upper_bound_finnish_horseshoe, color=\"magenta\", label=\"finnish\")\n",
    "#pr.plot_coefficients(theta_true, theta_lasso, color=\"red\", label=\"lasso\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
