After formally defining decision problems, we show that basic machine learning problems such as classification, regression, model choice, etc. are decision problems. 
Then we introduce subjective expected utility, the single unique guideline of all Bayesians, and go over its consequences for ML decisions.
Justifying subjective expected utility shall wait until Chapter~\ref{ch:foundations}.

For this chapter, we mostly used two references.
\citep{PaIn09} focuses on ideas and is a formidable entry point to (Bayesian) decision theory, while \citep{Sch12} is a great textbook-level reference for advanced reading, with mathematical details.

\section{ML problems are decision problems}

Formally, a decision problem is defined as 
\begin{enumerate}
    \item a set $\cS$ of \emph{states}. For technical reasons, we also require a $\sigma$-algebra $\Sigma_\cS$ that makes $(\cS, \Sigma_\cS)$ a Borel space \citep{Sch12}.
    \item a set of \emph{rewards} $\cR$. We also require a $\sigma$-algebra $\Sigma_\cR$, which should contain all singletons. 
    \item a set $\cA$ of measurable functions from $\cS$ to $\cR$, called \emph{actions}.
    \item a utility function $u:\cR \rightarrow \mathbb{R}$.
\end{enumerate}
We think of states as encoding all information about the situation at hand. Actions are what we are tasked to choose, and picking action $a$ while the situation is described by a given state $s$ leads to reward $a(s)$.
Note that we assume here that the same set of actions $\cA$ remains available in every state $s$.\footnote{In future versions of the course, we might make the framework more general, to include, e.g. Markov decision processes.} 

Most basic ML problems are of this kind; see Figure~\ref{f:decision_problems} for a few classical formalizations. 
Note that most choices made in this table are arbitrary, and correspond to the simplest variant of each problem. For instance, in classification, one might penalize false negatives and false positives differently; see Exercises.
Note also that, since \cite{Wal50} and as done in Section~\ref{s:linear_regression}, it is also customary to define loss functions instead of utilities, as $L(a,s) = -u(a(s))$. 
At this point of the document, both notations are as expressive, and we shall use them interchangeably.
The distinction will come later in Chapter~\ref{ch:foundations}, when we discuss state-dependent utilities.

\begin{figure}
    \centering
    \begin{tabular}{| c | c | c | c | c |}
        \hline
        & $\cS$ & $\cR$  & $\cA$ & $u(r)$ \\
        \hline
        \hline
        Regression & $(\cX\times \cY)^{n+1}$& $\cY=\mathbb{R}$ &   
        $\{a_g:s\mapsto y-g(x;x_{1:n},y_{1:n})\}$ & $-\Vert r\Vert^2$ \\
        Classification & $(\cX\times \cY)^{n+1}$  &$\cY=\{0,1\}$ &   
        $\{a_g:s\mapsto y-g(x;x_{1:n},y_{1:n})\}$ & $\IND{r = 0}$ \\
        Point estimation & $\cY^n\times\Theta$ & $\Theta$& $\{a_g:s\mapsto \theta-g(y_{1:n})\}$ & $- \Vert r\Vert^2$\\
        Interval estimation & $\cY^n\times\Theta$ & $\{0,1\}\times \mathbb{R}_+$ & $\{a_g:s\mapsto (\IND{\theta\in g(y_{1:n})}, \vert g(y_{1:n})\vert)\}$ &  $r_1+\gamma r_2$ \\
        Model choice & $\cY^n\times \left (\cup_{m=1}^M \{m\}\times\Theta_m \right )$ &  $\{0,1\}$ & $\{a_g:s\mapsto \IND{m= g(y_{1:n})}\}$ & $r$ \\
        \hline
    \end{tabular}
    \label{f:decision_problems}
    \caption{Some classical formalizations of ML problems as decision problems. Actions are labeled by functions $g$ (``predictors"), the domain and codomain of which should be obvious from the definition; for instance $g$ outputs a $\{0,1\}$ label in classification, and a Borel subset of $\Theta$ in ``interval" estimation.}
\end{figure}

\section{Bayesians maximize expected utility}

By definition, a Bayesian is someone who, facing a decision problem, picks a joint distribution $p$ over $(\cS, \Sigma_\cS)$, and chooses action
\begin{equation}
    a^\star \in \arg\max_{a\in\cA} \mathbb{E}_{s\sim p} u(a(s)).
    \label{e:expected_utility}
\end{equation}
The principle of decision-making encoded by \eqref{e:expected_utility} is called \emph{expected utility}, or \emph{subjective} expected utility, to insist on the fact that $p$ is an arbitrary choice from the decision maker.
At this stage, we have not discussed how Bayesians choose $p$, and there are many ways to do so; see Section~\ref{s:choosing_the_joint}.
Finally, to give an example of Bayesian decision, the ridge regression estimator is the Bayes action for a particular decision problem and joint distribution over states; see Section~\ref{s:linear_regression}.

In ML, it is customary to split the state variable into $(s_O,s_U)$, where $O, U\subset \{1,\dots,\text{dim} \cS\}$ are disjoint subsets that respectively index observed states (``data") and unknown states. 
In particular, we can rewrite \eqref{e:expected_utility} as
\begin{equation}
    a^\star \in \arg\max_{a\in\cA} \mathbb{E}_{s_O} \mathbb{E}_{s_U\vert s_O} u(a(s)).
    \label{e:expected_utility_tower}
\end{equation}
Now assume that actions are labeled by a ``predictor", which maps $s_O$ to one or several of the unknown variables of interest, say $s_I$ for some $I\subset U$. 
This is the case for all rows in Figure~\ref{f:decision_problems}. 
In classification or regression, for instance, the variable of interest is the new label $y$, and actions are labeled by measurable predictors of this variable of interest: evaluating $g(x; x_{1:n}, y_{1:n})$ is thought of as training a given algorithm (say, an SVM) over $\{(x_i,y_i), 1\leq i\leq n\}$ and evaluating the corresponding predictor at the new feature vector $x$.
Now, to maximize \eqref{e:expected_utility_tower} over $\cA$, it is enough maximize it over $g$. 
And since $g$ is a function of $s_O$ only, the optimal $g$ is 
\begin{equation}
    \label{e:posterior_expected_utility}
    g^\star: s_O \mapsto \arg\max_{g} \mathbb{E}_{s_U\vert s_O} u(a_g(s)).
\end{equation}
Indeed, by maximizing the innermost expectation in \eqref{e:expected_utility} for each fixed value of $s_O$, we maximize the whole expectation. 
The resulting $g^\star$ is called the \emph{Bayes decision rule}.

\section{Specifying a joint model}
We assume here familiarity with probabilistic graphical models, to the point of telling from a graph whether two sets of nodes are independent given a third one. 
The reader needing a recap is referred to \citep[Sections 10.1 to 10.5]{Mur12}.

\section{The Bayes decision rule for common ML problems}