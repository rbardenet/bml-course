\documentclass[9pt]{beamer}


\def\Title{Bayesian machine learning\\\vspace{3mm}
Bayesian deep learning}

\include{preamble}

\include{notations_julyan}
%\input{notations}

\usepackage{tikz}


\begin{document}
\begin{frame}
\maketitle
\end{frame}


\begin{frame}{Outline}
	\tableofcontents[pausesections,subsectionstyle=hide,subsubsectionstyle=hide]
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{What comes to \emph{your} mind when you hear ``Bayesian deep learning''?}
\end{frame}

%\begin{frame}{Deep neural networks}
%\begin{center}
%	\includegraphics[width=\textwidth]{figures_julyan/bnn}
%\end{center}
%\end{frame}


\begin{frame}{Deep neural networks Achilles heels}
\begin{center}
\only<1>{\includegraphics[height=.8\textheight,trim={12cm 6cm 12cm 8cm},clip]{figures_julyan/bdl/achilles1}}
\only<2>{\includegraphics[height=.8\textheight,trim={12cm 6cm 12cm 8cm},clip]{figures_julyan/bdl/achilles2}}
\only<3>{\includegraphics[height=.8\textheight,trim={12cm 6cm 12cm 8cm},clip]{figures_julyan/bdl/achilles3}}
\end{center}
\end{frame}

\begin{frame}{Different flavours of neural networks \citep{jospin2020handson}}
\begin{center}
\begin{tabular}{cc}
	\visible<1->{\includegraphics[width=.3\textwidth]{figures_julyan/bdl/hands-on/schemaNNPE}} &
	\visible<2->{\includegraphics[width=.3\textwidth]{figures_julyan/bdl/hands-on/schemaNNStochastic}}\\
	\visible<1->{Point estimate NN} & 
	\visible<2->{BNN w/ random weights}\\
	\visible<3->{\includegraphics[width=.3\textwidth]{figures_julyan/bdl/hands-on/schemaNNLastLayer}} &
	\visible<4->{\includegraphics[width=.3\textwidth]{figures_julyan/bdl/hands-on/schemaNNTransfert}}\\
	\visible<3->{BNN w/ last-layer rd weights} & 
	\visible<4->{BNN w/ random activations}
\end{tabular}
\end{center}
\end{frame}


\begin{frame}{Useful references}
\begin{itemize}
	\item \alert{First substantial reference}: Chapter 17 on BNNs by \fullcite{murphy2023probabilisticMLadvanced}
	\item  \alert{Review papers}: \citet{jospin2020hands,abdar2021review,goan2020bayesian,fortuin2021priors,ashukha2020pitfalls,band2021benchmarking,nado2021uncertainty}
	\item \alert{Our review paper}: \fullcite{arbel2023primer}
\end{itemize}
\end{frame}
	

%\begin{frame}{Bayesian neural networks}
%\begin{center}
%	\only<1>{%\hskip-3cm%\vspace{-2cm}
%	\includegraphics[width=1.1\textwidth]{figures_julyan/bnn1}}
%	\only<2>{\includegraphics[width=1.1\textwidth]{figures_julyan/bnn2}}
%	\only<3>{\includegraphics[width=1.1\textwidth]{figures_julyan/bnn3}}
%\end{center}
%\end{frame}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Feed-forward neural networks}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Neural networks notations}

\begin{figure}[ht!]
\begin{center}
\scalebox{.8}{
\input{figures_julyan/bdl_tex/NN-graph-intro}
}
\end{center}
%\caption{Neural network architecture.}
\label{figure:nn_visualization_intro}
\end{figure}

\begin{itemize}
	\item \alert{pre-nonlinearity} $\bg^{(\ell)}=\bg^{(\ell)}(\bx)$, \alert{post-nonlinearity}  $\bh^{(\ell)}=\bh^{(\ell)}(\bx)$
\begin{align*}\label{eq:propagation}
      \bg^{(\ell)}(\bx) = \bW^{(\ell)} \bh^{(\ell - 1)} (\bx), \quad \bh^{(\ell)} (\bx) = \phi(\bg^{(\ell)}(\bx))
\end{align*}
	\item \alert{nonlinearity} or \alert{activation function}  $\phi: \mathbb{R} \to \mathbb{R}$.
	\item \alert{weight matrix} $\bW^{(\ell)}$ of dimension $H_\ell\times H_{\ell-1}$ including a bias vector
\end{itemize}

\end{frame}


\begin{frame}{Training}
Optimization problem: minimize the loss function
\begin{equation*}
    \hat \bw = \argmin_{\bw} \mathcal{L}(\bw).
\end{equation*}
With gradient-based optimization: 
\begin{equation*}
    \bw \leftarrow \bw - \eta\, \partial_{\bw} \mathcal{L}(\bw).
\end{equation*}
$\eta > 0$ is a \alert{step size}, or \alert{learning rate}. Gradients are computed as products of gradients between each layer \alert{from right to left}, a procedure called \alert{backpropagation}~\citep{rumelhart1986learning}.

Gradients are approximated on randomly chosen subsets called \alert{batches}: stochastic gradient descent, SGD~\citep{robbins1951stochastic}. See survey of optimization methods by\citet{sun2019survey}.
\end{frame}



\begin{frame}{Architecture choice}

\begin{itemize}
	\item \alert{Convolutional neural networks (CNN)} are widely used in computer vision.
	\item \alert{Recurrent neural networks (RNN)} are advantageous for sequential data, designed to save the output of a layer by adding it back to the input~\citep{hochreiter1997long}.
	\item \alert{Residual neural networks (ResNet)} have residual blocks which add the output from the previous layer to the layer ahead, so-called \alert{skip-connections}~\citep{he2016deep}. Allows very deep  training.
\end{itemize}
\end{frame}



\begin{frame}{Expressiveness}
Expressiveness describes neural networks’ ability to approximate functions \citep{cybenko1989approximation, funahashi1989approximate, hornik1989multilayer,barron1994approximation}. 

\begin{block}{Universal approximation theorem}
	Neural networks of one hidden layer and suitable activation function can approximate any continuous function on a compact domain, say $f: [0,1]^N \to \mathbb{R}$, to any desired accuracy.
\end{block}

\alert{But} the size of such networks may be \alert{exponential in the input dimension $N$}, which makes them highly prone to overfitting as well as impractical.

Width-depth trade-offs studied by \citet{chatziafratis2020depth,chatziafratis2020better}.

\end{frame}



\begin{frame}[allowframebreaks]{Generalization and overfitting}
\alert{Classical regime}


%\begin{figure}[ht!]
\scalebox{.85}{
\input{figures_julyan/bdl_tex/classif-fitting}
}
%\caption{Illustration of the double-descent phenomenon.}
%\label{fig:double-descent}
%\end{figure}

\framebreak

\alert{Modern regime}\\
It was shown recently that when increasing the model size beyond the number of training examples, the model's test error can start \alert{decreasing again} after reaching the interpolation peak: \alert{double-descent} \citep{belkin2019reconciling}.

%\begin{figure}[ht!]
\scalebox{.8}{
\input{figures_julyan/bdl_tex/double-descent}
}
%\caption{Illustration of the double-descent phenomenon.}
%\label{fig:double-descent}
%\end{figure}
\end{frame}



\begin{frame}{Limitations with point-estimate neural networks}
\begin{itemize}
\item Inability to distinguish between \alert{in-domain} and \alert{out-of-domain} samples~\citep{lee2017training,mitros2019validity,hein2019relu,ashukha2020pitfalls}, and the sensitivity to \alert{domain shifts}~\citep{ovadia2019can}, which are explained in details later on;
\item Inability to provide reliable uncertainty estimates for a deep neural network’s decision and frequently occurring overconfident predictions~\citep{minderer2021revisiting};
\item Lack of transparency and interpretability of a deep neural network's inference model, which makes it difficult to trust their outcomes;
\item Sensitivity to adversarial attacks that make deep neural networks vulnerable for sabotage~\citep{wilson2016deep}.
\end{itemize}
\end{frame}


\begin{frame}{Great expectations of Bayesian neural networks}
\begin{itemize}
	\item Uncertainty quantification through the posterior distribution: BNN are shown to be better calibrated than NN
	\item Distinguishing between the epistemic uncertainty $p(\theta|D)$ and the aleatoric uncertainty $p(y|x,\theta)$: desirable in small dataset settings, providing high epistemic uncertainty for prediction, avoiding overfitting
	\item Integrating prior knowledge: most regularization methods for NN can be understood as setting a prior
	\item Interpreting known ML algorithms as approximate Bayesian methods: including regularization, ensembling, constant (learning rate) SGD, etc. 
\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Priors for neural networks}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Connection prior/initialization}

\begin{frame}{Connection prior/initialization}
	\alert{At initialization:}
	\begin{itemize}
		\item random weights and biases, e.g., $W^{(l)}_{ij} \sim \mathcal{N}(0, \sigma_w^2)$, 
		$B^{(l)}_{i} \sim \mathcal{N}(0, \sigma_b^2)$;
		\item inputs $X^{(1)}$ fixed.
	\end{itemize} 

	\bigskip
	
	\alert{Goal:}
	\begin{itemize}
		\item find a criterion that the pre-activations $Z^{(l)}$ should match;  \\
			example: $\mathrm{Var}(Z^{(l)}) = 1$;  
		\item deduce a constraint over the distributions of $W^{(l)}$ and $B^{(l)}$;  \\
			example: provided that $W^{(l)}_{ij} \sim \mathcal{N}(0, \sigma_w^2)$, 
			$B^{(l)}_{i} \sim \mathcal{N}(0, \sigma_b^2)$, tune $\sigma_w^2$ and $\sigma_b^2$ accordingly.
	\end{itemize} 
\end{frame}


\begin{frame}{Find a good initialization procedure}
	\alert{Naive heuristic.} (\emph{Understanding the difficulty of training deep feedforward neural 
	networks}, Glorot and Bengio 2010):
	\begin{itemize}
		\item idea: preserve the variance of the pre-activations $Z^{(l)}$;
		\item tune $\sigma_w^2$ and $\sigma_b^2$ s.t.: $\mathrm{Var}(Z^{(l + 1)}) = 
		\mathrm{Var}(Z^{(l)})$. 
	\end{itemize}
	Constraint: the NN is linear ($\phi = \mathrm{Id}$).  \\
	Result: $\sigma_b^2 = 0$, $\sigma_w^2 = 1$, $\mathrm{Var}(\frac{1}{\sqrt{n_l}} W_{ij}^l) = 1$.

	
	\bigskip
	
	\alert{Remark:} this heuristic can be extended to $\phi = $ReLU. \\
	(\emph{Delving deep into rectifiers: Surpassing human-level performance on imagenet classification}, He et al.,  
	2015)  \\
	Result: $\sigma_w^2 = 2$.
\end{frame}

\subsection{Neural-network Gaussian process (NN-GP), Gaussian hypothesis}

\begin{frame}{Neural-network Gaussian process (NN-GP)}
	\begin{itemize}
		\item 	An MLP with one hidden layer, whose width goes to infinity, and which has a Gaussian prior on all the parameters, converges to a Gaussian process with a well-defined kernel \citep{neal1996bayesian}.
	\end{itemize}
	\alert{Proof}: $H$ the number of hidden units, $\phi$ some nonlinear activation function, $b$ biais, weights $v$ and $\boldsymbol{u}$; then unit $k$ can be written
		$$f_k(\boldsymbol{x}) = b_k+\sum_{j=1}^H v_{jk}h_j(\boldsymbol{x}), \quad h_j(\boldsymbol{x}) = \phi(U_{0j}+\boldsymbol{x}^t \boldsymbol{u}_j)$$
		Then
		\begin{itemize}
			\item $\mathbb{E}[f_k(\boldsymbol{x})]=$
			\item $\mathbb{E}[f_k(\boldsymbol{x})f_k(\boldsymbol{x}')]=\ldots := \mathcal{K}(\boldsymbol{x},\boldsymbol{x}')$
			\item The joint distribution over $\{f_k(\boldsymbol{x_n}), n=1:N\}$ converges to a multivariate Gaussian.
			\item So the MLP converges to a GP with mean 0 and kernel $\mathcal{K}$, called the \alert{neural network kernel}. It is a non-stationary kernel.
		\end{itemize}
\end{frame}


\subsection{Neural tangent kernel (NTK)}

\begin{frame}{Neural tangent kernel (NTK)}
\begin{itemize}
	\item The NNGP is obtained under the assumption that weights are random and width goes to infinity.
	\item Natural question: can we derive a kernel from a DNN while it is being trained?
	\item The answer is yes \citep{jacot2018neural}. The associated kernel $\mathcal{T}(\boldsymbol{x},\boldsymbol{x}')$ is called the \alert{Neural tangent kernel} (NTK)
	$$\mathcal{T}(\boldsymbol{x},\boldsymbol{x}') := \nabla_{\boldsymbol{\theta}}f(\boldsymbol{x}; \boldsymbol{\theta_\infty})\cdot \nabla_{\boldsymbol{\theta}}f(\boldsymbol{x}'; \boldsymbol{\theta_\infty})$$
	and is obtained with 
	\begin{itemize}
		\item continuous time gradient descent
		\item letting the learning rate $\eta$ become infinitesimally small
		\item letting the widths go to infinity.
	\end{itemize}
\end{itemize}
\end{frame}



\subsection{Edge of Chaos}

\begin{frame}{Edge of Chaos framework}
\alert{See:} \citet{poole2016exponential} and \citet{schoenholz2016deep}

	\medskip
	
	\alert{Main idea.} Let $x_a$ and $x_b$ be two (fixed) inputs:
	\begin{itemize}
		\item variance: $v_a^{(l)} = \mathbb{E} [(Z_{j;a}^{(l)})^2]$;
		\item correlation: $c_{ab}^{(l)} = \frac{1}{\sqrt{v_a^{(l)} v_b^{(l)}}} \mathbb{E}[Z_{j;a}^{(l)} 
		Z_{j;b}^{(l)}]$; 
		\item goal: preserve the correlations $c_{ab}^{(l)}$ during propagation; 
		\item solution: find a recurrence equation $c_{ab}^{(l + 1)} = f(c_{ab}^{(l)})$; 
		\item to do so, we must make an assumption on the distribution of $Z_{j;a}^{(l)}$; \\
			$\Rightarrow$ Gaussian hypothesis: $Z_{j;a}^{(l)} \sim \mathcal{N}(0, v_a^{(l)})$; \\
			Central Limit Theorem ($n_{l} \rightarrow \infty$): $ Z^{(l + 1)}_{j;a} = \frac{1}{\sqrt{n_{l}}}W_j^{(l)} 
			X_a^{(l)} + B^{(l)}_j$; 
		\item resulting simplified dynamics: 
			\begin{align*}
				v_{a}^{(l + 1)}&= \mathcal{V}(v_{a}^{(l)} | \sigma_w, \sigma_b), & c_{ab}^{(l + 1)} &= 
				\mathcal{C}(c_{ab}^{(l)}, v_a^{(l)}, v_b^{(l)} | \sigma_w, \sigma_b) .
			\end{align*}
	\end{itemize}
\end{frame}

\begin{frame}{Edge of Chaos results (Poole et al., 2016; Schoenholz et al., 2016)}
	\alert{Additional assumptions.}
	\begin{itemize}
		\item the sequence $(v_a^{(l)})_l$ tends to a non-zero limit $v^*$, independent from the starting point 
		$v_a^{(0)}$;
		\item $(v_a^{(l)})_l$ is assumed to have already converged; 
		\item so: $c_{ab}^{(l + 1)} = \mathcal{C}(c_{ab}^{(l)}, v^*, v^* | \sigma_w, \sigma_b) = 
		\mathcal{C}_*(c_{ab}^{(l)} | \sigma_w, \sigma_b)$.
	\end{itemize} 
	
	\medskip
	
	\alert{Phases of information propagation:}
	\begin{itemize}
		\item \emph{chaotic phase}: $\lim_{l \rightarrow \infty} c_{ab}^{l} = c^{*} < 1$ \\
			$\Rightarrow$ decorrelate (partially or fully);
		\item \emph{ordered phase}: $\lim_{l \rightarrow \infty} c_{ab}^{l} = c^{*} = 1$ with 
		$\mathcal{C}_*'(1) < 1$ \\
			$\Rightarrow$ correlate fully at an exponential rate;
		\item \emph{edge of chaos}: $\lim_{l \rightarrow \infty} c_{ab}^{l} = c^{*} = 1$ with 
		$\mathcal{C}_*'(1) = 1$ \\
			$\Rightarrow$ correlate fully at sub-exponential rate. 
	\end{itemize}
	$\Rightarrow$ tune $\sigma_w$ and $\sigma_b$ such that the NN lies in the ``edge of chaos'' phase.
\end{frame}


\begin{frame}[allowframebreaks]{Edge of Chaos}
\citet{poole2016exponential} and  \citet{schoenholz2016deep} \begin{figure}[h]
\begin{center}
\includegraphics[width=\linewidth]{figures_julyan/bdl/figure_1}
\caption{(a) Edge of chaos diagram showing the boundary between ordered and chaotic phases as a function of $\sigma_w^2$ and $\sigma_b^2$. (b) The residual $|q^* - q_{aa}^l|$ as a function of depth on a log-scale with $\sigma_b^2 = 0.05$ and $\sigma_w^2$ from 0.01 (red) to 1.7 (purple). Clear exponential behavior is observed. (c) The residual $|c^* - c_{ab}^l|$ as a function of depth on a log-scale. Again, the exponential behavior is clear. The same color scheme is used here as in (b).  \label{fig:phase_diagram}}
\end{center}
\hfill\textcolor{gray}{From \citet{schoenholz2016deep}}
\end{figure}


\end{frame}


\subsection{Unit priors get heavier with depth}

\begin{frame}{Understanding priors at the unit level}
	\begin{definition}[Generalized Weibull-tail on $\mathbb{R}$]
\label{def:gen_weibull-tail_r}
A random variable $X$ is generalized Weibull-tail on $\mathbb{R}$ with tail parameter $\beta > 0$ if both its right and left tails are upper and lower bounded by some Weibull-tail functions with tail parameter~$\beta$:
\begin{align*}
    \edr^{-x^{\beta} l_1^r(x)} \le & \overline{F}_X(x) \le \edr^{-x^{\beta} l_2^r(x)}, \quad \ \ \ \text{for }x > 0 \text{ and $x$ large enough}, \\
    \edr^{-|x|^{\beta} l_1^l(|x|)} \le & F_X(x) \le \edr^{-|x|^{\beta} l_2^l(|x|)}, \quad \text{for } x < 0 \text{ and $-x$ large enough},
\end{align*}
where $l_1^r$, $l_2^r$, $l_1^l$ and $l_2^l$ are slowly-varying functions. 
We note $X \sim GWT(\beta)$. 
\end{definition}
\end{frame}

\begin{frame}{Understanding priors at the unit level}
This tail description reveals the difference between hidden units' distributional properties in finite- and infinite-width Bayesian neural networks, since 
hidden units are generalized Weibull-tail with a tail parameter depending on those of the weights:
\begin{theorem}[\citealp{vladimirova2021accurate}]
\label{theorem:hidden_units_are_gwt}
Consider a Bayesian neural network  with ReLU activation function. Let $\ell$-th layer weights be independent symmetric generalized Weibull-tail on~$\mathbb{R}$ with tail parameter $\beta^{(\ell)}_w$. 
Then conditional on the input $\bx$, the marginal prior distribution induced by forward propagation on any pre-activation is generalized Weibull-tail on~$\mathbb{R}$: for any $1\leq \ell\leq L$, and for any $1\leq m\leq H_\ell$,
$$g_m^{(\ell)}\sim GWT(\beta^{(\ell)}),$$
with tail parameter $\beta^{(\ell)}$ such that $\frac{1}{\beta^{(\ell)}} = \frac{1}{\beta^{(1)}_w} + \dots + \frac{1}{\beta^{(\ell)}_w}$,
where a $GWT$ distribution is defined in Definition~\ref{def:gen_weibull-tail_r}.
\end{theorem}
Note that the most popular case of weight prior, iid Gaussian \citep{neal1996bayesian}, corresponds to $\text{GWT}_{\mathbb{R}}(2)$ weights. This leads to units of layer $\ell$ which are $\text{GWT}_{\mathbb{R}}(\frac{2}{\ell})$. 
\end{frame}

\begin{frame}{Understanding priors at the unit level}


\def\bW{\boldsymbol{W}}
\def\bU{\boldsymbol{U}}
\def\Lnorm{\mathcal{L}}

\begin{center}
\begin{tabular}{@{}cclc@{}}
\toprule
Layer                         & Penalty on $\bW$         & \multicolumn{2}{c}{Approximate penalty on $\bU$}         \\ \toprule
$1$ & $\Vert \bW^{(1)}\Vert_2^{2}$, $\Lnorm^2$   & $\Vert \bU^{(1)}\Vert_2^{2}$ & $\Lnorm^2$  (weight decay)\\%\hline
$2$ & $\Vert \bW^{(2)}\Vert_2^{2}$, $\Lnorm^2$   & $\Vert \bU^{(2)}\Vert$ & $\Lnorm^1$  (Lasso)\\%\hline
% \vdots &\vdots &\vdots \\
$\ell$ & $\Vert \bW^{(\ell)}\Vert_2^{2}$, $\Lnorm^2$   & $\Vert \bU^{(\ell)}\Vert_{2/\ell}^{2/\ell}$ & $\Lnorm^{2/\ell}$ \\ 
% $\ell = L$ & $\Vert \bW^{(L)}\Vert_2^{2}$, $\Lnorm^2$   & $\Vert \bU^{(L)}\Vert_{2/L}^{2/L}$, $\Lnorm^{2/L}$ \\ 
\bottomrule
\end{tabular}\bigskip 

  \includegraphics[width=.32\textwidth]{figures_julyan/bdl/001}
  \includegraphics[width=.32\textwidth]{figures_julyan/bdl/002}
  \includegraphics[width=.32\textwidth]{figures_julyan/bdl/003}

\end{center}

\end{frame}

\subsection{Other priors}



\begin{frame}{Other priors}
\begin{itemize}
	\item Although Gaussian priors are simple and widely used, they are not the only option. 
	\item For some applications, it is useful to use shrinkage or sparsity promoting priors which encourage most of the weights to be small/close to zero. Examples of such priors
	\begin{itemize}
	 \item  Laplace prior
	$$p_\lambda(x) = \frac{\lambda}{2}e^{-\lambda|x|}.$$
		\item spike-and-slab
	\end{itemize}
\end{itemize}

\end{frame}






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Inference algorithms}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	
\end{frame}

\begin{frame}{Bayesian inference: basic sampling algorithm}
Denote data by $D=\{D_x,D_y\}$ and parameters (weights) by $\boldsymbol{\theta}$.
\begin{center}
	\includegraphics[width=.8\textwidth]{figures_julyan/bdl/hands-on/algo1}
\end{center}
\end{frame}



\begin{frame}{Laplace approximation}
Computes a Gaussian approximation to the posterior centered at the MAP estimate

\begin{itemize}
	\item It is simple
	\item But... computing the Hessian is expensive, and may result in a non-positive definite matrix since the log likelihood of deep neural networks is non-convex.
	\item Gauss--Newton approximation to the Hessian
\end{itemize}

\begin{block}{Generalized Gauss--Newton approximation}
	$$\bH^{\mathrm{GGN}}:= \sum_{i=1}^n \mathcal{J}_{\bw}(\bx_i)^{\top}\boldsymbol{\Lambda}(\by_i;f_i)\mathcal{J}_{\bw}(\bx_i),$$
where $\mathcal{J}_{\bw}(\bx)$ is the network per-sample Jacobian $\left[\mathcal{J}_{\bw}(\bx)\right]_{c}=\nabla_{\bw}f_c(\bx;\bw_{\hat{\rho}})$, and $\boldsymbol{\Lambda}(\by;f)=-\nabla^2_{ff}\log p(\by;f)$ is the per-input noise matrix.% \citep{kunstner2019limitations}.
\end{block}

\end{frame}




\begin{frame}{Bayesian neural networks: early algorithms}
\begin{itemize}[<+->]
	\item \alert{Markov chain Monte Carlo} (MCMC), \alert{Hamiltonian Monte Carlo} (HMC). No-U-Turn sampler (NUTS) it most often used in probabilistic programming languages (Stan, PyMC3, Pyro, etc): is improves over classic HMC by allowing hyperparameters to be set automatically instead of manually
	\item \alert{Variational inference} (VI): scales better than MCMC algorithms. Idea: find an approximate variational distribution in a variational family that is as close as possible to the exact posterior by minimizing the Kullback--Leibler divergence. Turns sampling into optimization.
	\item \alert{Stochastic variational inference} (SVI): scales better than VI, stochastic gradient descent method applied to VI. Gradient of objective is computed only on mini-batches.
\end{itemize}
\end{frame}




\begin{frame}{Bayesian neural networks: early algorithms}
\begin{alertblock}{BUT}
	Stochasticity in gradient estimation stops backpropagation from functioning
\end{alertblock}
\pause


\begin{block}{Tricks for Monte Carlo gradient estimation}
	A number of \alert{tricks} \citep[see][]{mohamed2020montecarlo}
	\begin{itemize}
		\item Log-derivative trick: score function estimators
		\item Reparameterisation trick: pathwise derivative estimator
		\item Measure-valued gradient estimators
	\end{itemize}
\end{block}
\end{frame}



\begin{frame}{Bayesian neural networks: adapted algorithms}
\begin{itemize}
	\item \alert{Bayes-by-backprop} (BBB) and \alert{probabilistic backpropagation} (PBP): implement the reparameterisation trick
	\item \alert{Monte Carlo dropout}: turning dropout into an approximate Bayesian algorithm (variational inference)
	\item  \alert{Bayes via stochastic gradient descent}: includes MCMC algorithms based on the SGD dynamic such as stochastic gradient Langevin dynamic (SGLD) and Variational Inference based on SGD dynamic such as ensembling
\end{itemize}
\end{frame}


\begin{frame}[allowframebreaks]{Monte Carlo dropout}
Dropout technique reinterpreted as a form of approximate Bayesian variational inference \citep{kingma2015variational,gal2016dropout}.

\alert{Idea}: performing random sampling at test time. Instead of turning off the dropout layers at test time (as is usually done), \textcolor{orange2}{hidden units} are randomly dropped out according to a \textcolor{orange2}{Bernoulli$(p)$} distribution. Repeating this operation $M$ times provides $M$ versions of the MAP estimate of the network parameters $\bw^m$, $m=1,\ldots,M$ (where some units of the MAP are dropped), yielding an approximate posterior predictive in the form of the equal-weight average:
\begin{equation*}
%\label{eq:MCdropout_post_pred}
	p(y\vert x, \mathcal{D}^n)\approx \frac{1}{M}\sum_{m=1}^M p(y\vert x, \bw^m).
\end{equation*}

\vfill 

\begin{itemize}
	\item Monte Carlo dropout captures some uncertainty from out-of-distribution (OOD) inputs
	\item But... \alert{does not provide valid posterior uncertainty}
	\item \citet{folgoc2021mc} show that the Monte Carlo dropout posterior predictive assigns \alert{zero probability} to the true model posterior predictive distribution
\end{itemize}
\end{frame}


\begin{frame}{Tempered and cold posteriors}
\begin{block}{Tempered posterior}
	A \alert{tempered posterior distribution} with \alert{temperature parameter} $\alert{T}>0$ is defined as $$p(\bw|D) \propto \exp(U (\bw)/\alert{T} )$$ where $U(\bw)$ is the posterior energy function
\begin{equation*}
 U(\bw) :=  \log p(\mathcal{D} | \bw) + \log p(\bw),
\end{equation*}
\end{block}

\pause

\begin{block}{Cold posterior effect}
Empirical evidence \citep{wenzel2020good} that posteriors  exponentiated to some power greater than one (or, equivalently, dividing the energy function $U(\bw)$ by some temperature $T<1$), \alert{performs better} than an untempered one.
\end{block}



\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Performance metrics}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}{Performance metrics}
\begin{itemize}
	\item \alert{Predictive performance}: ability of the model to give correct answers. Based on metrics, eg: mean square error, risk of 0-1 loss for classification task.
	\item \alert{Model calibration}: assessing that the network is neither overconfident nor underconfident about its prediction. Requires using a test set. Eg: expected calibration error (ECE).
\end{itemize}
\end{frame}


\section*{References}
\setbeamertemplate{bibliography item}[text]%,
\begin{frame}[allowframebreaks]
\frametitle{References}
\small
\printbibliography
\normalsize
\end{frame}
\end{document}
